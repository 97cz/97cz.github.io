<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Python简单爬虫 | 知行记</title><meta name="keywords" content="Python"><meta name="author" content="小辑轻舟"><meta name="copyright" content="小辑轻舟"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="简介 网络爬虫，也叫 网络蜘蛛（Web Spider），它根据网页地址（URL）爬取网页内容。  URL 专业一些的叫法是 统一资源定位符（Uniform Resource Locator），它的一般格式如下（带方括号[]的为可选项） protocol :&#x2F;&#x2F; hostname[:port] &#x2F; path &#x2F; [;parameters][?query]#fragment   URL 的格式主要由前">
<meta property="og:type" content="article">
<meta property="og:title" content="Python简单爬虫">
<meta property="og:url" content="http://example.com/2022/09/15/Python%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="知行记">
<meta property="og:description" content="简介 网络爬虫，也叫 网络蜘蛛（Web Spider），它根据网页地址（URL）爬取网页内容。  URL 专业一些的叫法是 统一资源定位符（Uniform Resource Locator），它的一般格式如下（带方括号[]的为可选项） protocol :&#x2F;&#x2F; hostname[:port] &#x2F; path &#x2F; [;parameters][?query]#fragment   URL 的格式主要由前">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2022-09-15T00:56:12.000Z">
<meta property="article:modified_time" content="2022-09-15T01:48:27.596Z">
<meta property="article:author" content="小辑轻舟">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/09/15/Python%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Python简单爬虫',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-15 09:48:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">知行记</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Python简单爬虫</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-15T00:56:12.000Z" title="发表于 2022-09-15 08:56:12">2022-09-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-15T01:48:27.596Z" title="更新于 2022-09-15 09:48:27">2022-09-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Python简单爬虫"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ol>
<li><p>网络爬虫，也叫 <code>网络蜘蛛</code>（Web Spider），它根据网页地址（URL）爬取网页内容。</p>
</li>
<li><p>URL 专业一些的叫法是 <code>统一资源定位符</code>（Uniform Resource Locator），它的一般格式如下（带方括号[]的为可选项）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">protocol :// hostname[:port] / path / [;parameters][?query]#fragment</span><br></pre></td></tr></table></figure>

<blockquote>
<p>URL 的格式主要由前个三部分组成</p>
<ul>
<li><code>protocol</code>：第一部分就是协议，例如百度使用的就是https协议；</li>
<li><code>hostname[:port]</code>：第二部分就是主机名（还有端口号为可选参数），一般网站默认的端口号为80，例如百度的主机名就是<code>www.baidu.com</code>，这个就是服务器的地址；</li>
<li><code>path</code>：第三部分就是主机资源的具体地址，如目录和文件名等。</li>
</ul>
</blockquote>
<p>  <em>注：通俗一点讲，URL 就是每个服务器的地址。</em></p>
</li>
</ol>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><ol>
<li><p>网络爬虫的第一步就是根据 URL 获取网页的 HTML 信息。在 Python3 中，可以使用 urllib.request 和 requests 进行网页爬取。</p>
<blockquote>
<ul>
<li>urllib 库是 Python 内置的，无需额外安装，只要安装了 Python 就可以使用这个库。</li>
<li><code>requests</code> 库是第三方库，需要自己安装——<code>pip install requests</code>。</li>
</ul>
</blockquote>
</li>
<li><p>requests 库的基础方法如下</p>
<img src="\img\python\image-20220908175012465.png" alt="image-20220908175012465" style="zoom:67%;" />

<p>其中用的最多的就是 <code>requests.get()</code> 方法，它用于向服务器发起 GET 请求，获取数据。requests.get() 方法必须设置的一个参数就是 <code>url</code> ，因为我们得告诉 GET 请求我们要获取谁的信息。实例：爬取百度翻译</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    target = <span class="string">&quot;http://fanyi.baidu.com/&quot;</span></span><br><span class="line">    req = requests.get(url = target)</span><br><span class="line">    req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(req.text)</span><br></pre></td></tr></table></figure>
</li>
<li><p>爬虫步骤</p>
<blockquote>
<ul>
<li>发起请求：我们需要先明确如何发起 HTTP 请求，获取到数据。</li>
<li>解析数据：获取到的数据乱七八糟的，我们需要提取出我们想要的数据。解析数据工具有很多，比如xpath、Beautiful Soup、正则表达式等。</li>
<li>保存数据：将我们想要的数据，保存下载。</li>
</ul>
</blockquote>
</li>
<li><p>安装<strong>beautifulsoup4</strong>以及<strong>lxml</strong>。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install beautifulsoup4</span><br><span class="line">pip install lxml</span><br></pre></td></tr></table></figure>

<p>Beautiful Soup官方中文教程：<a target="_blank" rel="noopener" href="https://beautifulsoup.readthedocs.io/zh_CN/latest/">https://beautifulsoup.readthedocs.io/zh_CN/latest/</a></p>
</li>
</ol>
<h2 id="实例：爬取小说"><a href="#实例：爬取小说" class="headerlink" title="实例：爬取小说"></a>实例：爬取小说</h2><h3 id="爬取网页内容"><a href="#爬取网页内容" class="headerlink" title="爬取网页内容"></a>爬取网页内容</h3><p>使用Beautiful Soup 提取正文内容并清洗数据，编写代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    target = <span class="string">&#x27;http://book.zongheng.com/chapter/672340/36898237.html&#x27;</span></span><br><span class="line">    req = requests.get(url = target)</span><br><span class="line">    req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    html = req.text</span><br><span class="line">    </span><br><span class="line">    bs = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    texts = bs.find(<span class="string">&#x27;div&#x27;</span>, &#123;<span class="string">&#x27;class&#x27;</span>:<span class="string">&#x27;content&#x27;</span>&#125;)</span><br><span class="line">    <span class="comment"># 此处写成print(texts.text.strip())即可</span></span><br><span class="line">    <span class="built_in">print</span>(texts.text.strip().split(<span class="string">&#x27;\xa0&#x27;</span>*<span class="number">4</span>))</span><br></pre></td></tr></table></figure>

<p>注：<em>texts.text方法提取所有文字，strip方法去掉回车，split方法根据’\xa0’切分数据，此处有4个空格，因此’\xa0’*4（\xa0是不间断空白符<code>&amp;nbsp;</code>），即去掉多余的空格。</em></p>
<p>注：<em>可根据属性来查找元素：<code>content = html.find(attrs = &#123;&#39;class&#39;:&#39;class属性值&#39;,&#39;id&#39;:&#39;id属性值&#39;&#125;)</code></em></p>
<img src="\img\python\image-20220908181221793.png" alt="image-20220908181221793" style="zoom:75%;" />



<h3 id="获取章节链接及章节名"><a href="#获取章节链接及章节名" class="headerlink" title="获取章节链接及章节名"></a>获取章节链接及章节名</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">server = <span class="string">&#x27;https://m.31xiaoshuo.com&#x27;</span></span><br><span class="line">target = <span class="string">&#x27;https://m.31xiaoshuo.com/12/12970/index.htm&#x27;</span></span><br><span class="line">req = requests.get(url = target)</span><br><span class="line">req.encoding = req.apparent_encoding</span><br><span class="line">bs = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">chapters = bs.find(<span class="string">&#x27;ul&#x27;</span>,&#123;<span class="string">&#x27;class&#x27;</span>:<span class="string">&#x27;am-list-striped&#x27;</span>&#125;)</span><br><span class="line">chapters = chapters.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> chapter <span class="keyword">in</span> chapters:</span><br><span class="line">    <span class="comment"># chapter.get(&#x27;href&#x27;)方法获取章节地址</span></span><br><span class="line">    url = server + chapter.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    <span class="comment"># chapter.string方法获取章节名</span></span><br><span class="line">    name = chapter.string</span><br><span class="line">    <span class="built_in">print</span>(url+<span class="string">&#x27; &#x27;</span>+ name)</span><br></pre></td></tr></table></figure>

<img src="\img\python\image-20220908195022684.png" alt="image-20220908195022684" style="zoom:80%;" />



<h3 id="整合代码"><a href="#整合代码" class="headerlink" title="整合代码"></a>整合代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">target</span>):</span><br><span class="line">    req = requests.get(url = target)</span><br><span class="line">    req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    html = req.text</span><br><span class="line">    bs = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    texts = bs.find(<span class="string">&#x27;div&#x27;</span>, &#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;content&#x27;</span>&#125;)</span><br><span class="line">    content = texts.text.strip().split(<span class="string">&#x27;\xa0&#x27;</span>*<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    server = <span class="string">&#x27;https://www.biqupai.com&#x27;</span></span><br><span class="line">    book_name = <span class="string">&#x27;全球高武.txt&#x27;</span></span><br><span class="line">    target = <span class="string">&#x27;https://www.biqupai.com/81_81336/&#x27;</span></span><br><span class="line">    req = requests.get(url = target)</span><br><span class="line">    req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    html = req.text</span><br><span class="line">    bs = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    chapters = bs.find(<span class="string">&#x27;div&#x27;</span>, &#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;list&#x27;</span>&#125;)</span><br><span class="line">    chapters = chapters.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> chapter <span class="keyword">in</span> tqdm(chapters):</span><br><span class="line">        <span class="comment"># 使用了tqdm显示下载进度</span></span><br><span class="line">        chapter_name = chapter.string</span><br><span class="line">        url = server + chapter.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">        content = get_content(url)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(book_name,<span class="string">&#x27;a&#x27;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(chapter_name)</span><br><span class="line">            f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            f.write(<span class="string">&#x27;\n&#x27;</span>.join(content))</span><br><span class="line">            f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="实例：爬取漫画"><a href="#实例：爬取漫画" class="headerlink" title="实例：爬取漫画"></a>实例：爬取漫画</h2><h3 id="获取章节名和章节链接"><a href="#获取章节名和章节链接" class="headerlink" title="获取章节名和章节链接"></a>获取章节名和章节链接</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    target = <span class="string">&#x27;https://www.dmzj.com/info/yaoshenji.html&#x27;</span></span><br><span class="line">    req = requests.get(url = target)</span><br><span class="line">    req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    bs = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    list_con_li = bs.find(<span class="string">&#x27;ul&#x27;</span>,&#123;<span class="string">&#x27;class&#x27;</span>:<span class="string">&#x27;list_con_li&#x27;</span>&#125;)</span><br><span class="line">    comic_list = list_con_li.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    chapter_names = []</span><br><span class="line">    chapter_urls = []   </span><br><span class="line">    <span class="keyword">for</span> comic <span class="keyword">in</span> comic_list:</span><br><span class="line">        href = comic.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">        <span class="comment"># 此处由于a标签中嵌套了span标签，因此不能直接使用.string</span></span><br><span class="line">        <span class="comment"># .text获取标签中的文本</span></span><br><span class="line">        name = comic.text</span><br><span class="line">        <span class="comment">#ls.insert(i,x)：在列表ls的第i位置增加元素x，其余元素向后移</span></span><br><span class="line">        chapter_names.insert(<span class="number">0</span>,name)</span><br><span class="line">        chapter_urls.insert(<span class="number">0</span>,href)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(chapter_names)</span><br><span class="line">    <span class="built_in">print</span>(chapter_urls)  </span><br></pre></td></tr></table></figure>



<h3 id="获取漫画图片地址"><a href="#获取漫画图片地址" class="headerlink" title="获取漫画图片地址"></a>获取漫画图片地址</h3><p>打开第一章的链接，发现链接后面自动添加了 <code>#@page=1</code>。翻页会发现，第二页的链接是后面加了 <code>#@page=2</code>，以此类推。这些并不是图片的地址，而是这个展示页面的地址，要下载图片，首先要拿到图片的 <code>真实地址</code>。</p>
<p>审查元素找图片地址，你会发现，这个页面 <code>不能右键</code>！有些甚至把F12禁用了。面对这种禁止看页面源码的初级手段，一个优雅的通用解决办法是，在连接前加个<code>view-source:</code>，此时看到的就是 <code>网页源码</code>。例如：<code>view-source:https://www.dmzj.com</code></p>
<p>更简单的办法是，将鼠标焦点放在浏览器地址栏，然后按下 <code>F12</code> 依然可以调出调试窗口。我们可以在浏览器调试窗口中的 <code>Network</code> 里找到这个页面加载的内容，例如一些css文件、js文件、图片等等。在 <code>Network</code> 中可以很轻松地找到我们想要的图片 <code>真实地址</code>，调试工具很强大，Headers可以看一些请求头信息，Preview可以浏览返回信息。</p>
<p>好了，拿到了图片的真实地址，我们看下链接：</p>
<p><code>https://images.dmzj.com/img/chapterpic/3059/14237/14395217739069.jpg</code></p>
<p>这就是图片的真实地址，拿着这个链接去html页面中搜索，看下它存放在哪个<code>img</code>标签里了，搜索一下你会发现，浏览器中的html页面是有这个图片链接的。</p>
<img src="\img\python\image-20220908200544171.png" alt="image-20220908200544171" style="zoom:80%;" />

<p>但是用<code>view-source:</code>打开这个页面，你会发现你搜索不到这个图片链接。说明这个图片是 <code>动态加载</code> 的！</p>
<p>使用<code>view-source:</code>方法，可以看页面源码，并不管动态加载的内容。这里面没有图片链接，就说明图片是动态加载的。使用JavaScript动态加载，无外乎两种方式：外部加载、内部加载。</p>
<ul>
<li><p>外部加载：在html页面中，以引用的形式，加载一个js，例如：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;script type=<span class="string">&quot;text/javascript&quot;</span> src=<span class="string">&quot;call.js&quot;</span>&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>内部加载：Javascript脚本内容写在html内，例如这个漫画网站。</p>
<img src="\img\python\image-20220908200937209.png" alt="image-20220908200937209" style="zoom:80%;" /></li>
</ul>
<p>这时候，就可以用搜索功能了，教一个搜索小技巧。</p>
<p><code>https://images.dmzj.com/img/chapterpic/3059/14237/14395217739069.jpg</code></p>
<p>图片链接是这个，那就用图片的名字去掉后缀，也就是<code>14395217739069</code>在浏览器的调试页面搜索，因为一般这种动态加载，链接都是程序合成的，搜它准没错！</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;script type=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><br><span class="line">        <span class="keyword">var</span> arr_img = <span class="keyword">new</span> <span class="title class_">Array</span>();</span><br><span class="line">        <span class="keyword">var</span> page = <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">        <span class="built_in">eval</span>(<span class="keyword">function</span>(<span class="params">p,a,c,k,e,d</span>)&#123;e=<span class="keyword">function</span>(<span class="params">c</span>)&#123;<span class="keyword">return</span>(c&lt;a?<span class="string">&#x27;&#x27;</span>:<span class="title function_">e</span>(<span class="built_in">parseInt</span>(c/a)))+((c=c%a)&gt;<span class="number">35</span>?<span class="title class_">String</span>.<span class="title function_">fromCharCode</span>(c+<span class="number">29</span>):c.<span class="title function_">toString</span>(<span class="number">36</span>))&#125;;<span class="keyword">if</span>(!<span class="string">&#x27;&#x27;</span>.<span class="title function_">replace</span>(<span class="regexp">/^/</span>,<span class="title class_">String</span>))&#123;<span class="keyword">while</span>(c--)&#123;d[<span class="title function_">e</span>(c)]=k[c]||<span class="title function_">e</span>(c)&#125;k=[<span class="keyword">function</span>(<span class="params">e</span>)&#123;<span class="keyword">return</span> d[e]&#125;];e=<span class="keyword">function</span>(<span class="params"></span>)&#123;<span class="keyword">return</span><span class="string">&#x27;\\w+&#x27;</span>&#125;;c=<span class="number">1</span>&#125;;<span class="keyword">while</span>(c--)&#123;<span class="keyword">if</span>(k[c])&#123;p=p.<span class="title function_">replace</span>(<span class="keyword">new</span> <span class="title class_">RegExp</span>(<span class="string">&#x27;\\b&#x27;</span>+<span class="title function_">e</span>(c)+<span class="string">&#x27;\\b&#x27;</span>,<span class="string">&#x27;g&#x27;</span>),k[c])&#125;&#125;<span class="keyword">return</span> p&#125;(<span class="string">&#x27;g f=\&#x27;&#123;&quot;e&quot;:&quot;h&quot;,&quot;i&quot;:&quot;0&quot;,&quot;l&quot;:&quot;k\\/3\\/5\\/2\\/j.4\\r\\6\\/3\\/5\\/2\\/d.4\\r\\6\\/3\\/5\\/2\\/7.4\\r\\6\\/3\\/5\\/2\\/8.4\\r\\6\\/3\\/5\\/2\\/c.4\\r\\6\\/3\\/5\\/2\\/b.4\\r\\6\\/3\\/5\\/2\\/a.4\\r\\6\\/3\\/5\\/2\\/9.4\\r\\6\\/3\\/5\\/2\\/m.4\\r\\6\\/3\\/5\\/2\\/v.4\\r\\6\\/3\\/5\\/2\\/A.4\\r\\6\\/3\\/5\\/2\\/n.4\\r\\6\\/3\\/5\\/2\\/B.4\\r\\6\\/3\\/5\\/2\\/x.4\\r\\6\\/3\\/5\\/2\\/y.4&quot;,&quot;w&quot;:&quot;p&quot;,&quot;o&quot;:&quot;1&quot;,&quot;q&quot;:&quot;\\s\\u \\t\\z&quot;&#125;\&#x27;;&#x27;</span>,<span class="number">38</span>,<span class="number">38</span>,<span class="string">&#x27;||14237|chapterpic|jpg|3059|nimg|14395217891719|14395217893745|14395217913416|14395217908431|14395217904781|1439521790086|1439521788936|id|pages|var|41917|hidden|14395217739069|img|page_url|14395217918734|14395217931135|chapter_order|15|chapter_name||u7b2c01|u91cd|u8bdd|14395217923415|sum_pages|14395217940216|14395217943921|u751f|14395217926321|1439521793602&#x27;</span>.<span class="title function_">split</span>(<span class="string">&#x27;|&#x27;</span>),<span class="number">0</span>,&#123;&#125;))</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure>

<p>不出意外，你就能看到这段代码，<code>14395217739069</code>就混在其中！</p>
<p><code>https://images.dmzj.com/img/chapterpic/3059/14237/14395217739069.jpg</code> 链接中的数字是不是眼熟？</p>
<img src="\img\python\image-20220908202122458.png" alt="image-20220908202122458" style="zoom:67%;" />

<p>链接中的数字是这几个数字合成的。好了，我有个大胆的想法！直接把这些长的数字搞出来，合成下链接试试看。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.dmzj.com/view/yaoshenji/41917.html&#x27;</span></span><br><span class="line">req = requests.get(url = url)</span><br><span class="line">html = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">script_info = html.script</span><br><span class="line"><span class="comment">#找到所有长度为14的数字</span></span><br><span class="line">pics = re.findall(<span class="string">&#x27;\d&#123;13,14&#125;&#x27;</span>,<span class="built_in">str</span>(script_info))</span><br><span class="line"><span class="comment"># 找到3059</span></span><br><span class="line">chapter_qian = re.findall(<span class="string">&#x27;\|jpg\|(\d&#123;4&#125;)&#x27;</span>,<span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 找到14237</span></span><br><span class="line">chapterpic_hou = re.findall(<span class="string">&#x27;\|\|(\d&#123;5&#125;)&#x27;</span>,<span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> pic <span class="keyword">in</span> pics:</span><br><span class="line">    url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/&#x27;</span> + chapter_qian + <span class="string">&#x27;/&#x27;</span> + chapterpic_hou + <span class="string">&#x27;/&#x27;</span> + pic + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(url)</span><br></pre></td></tr></table></figure>

<p><img src="/%5Cimg%5Cpython%5Cimage-20220908202457535.png" alt="image-20220908202457535"></p>
<p>比对一下你会发现，这些，还真就是漫画图片的链接！但是有个问题，这么合成的的图片链接 <code>不是按照漫画顺序</code> 的，这下载下来漫画图片都是乱的啊！<code>不优雅</code>！</p>
<p>这个网站也是人写的嘛！是人，就好办！惯性思维，要是你，是不是小数放在前面，大数放在后面？这些长的数字里，有13位的，有14位的，并且都是以14开头的数字，那我就赌它 <code>末位补零后比较</code> 的结果，就是图片的顺序！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.dmzj.com/view/yaoshenji/41917.html&#x27;</span></span><br><span class="line">req = requests.get(url = url)</span><br><span class="line">html = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">script_info = html.script</span><br><span class="line">pics = re.findall(<span class="string">&#x27;\d&#123;13,14&#125;&#x27;</span>,<span class="built_in">str</span>(script_info))</span><br><span class="line"><span class="comment"># 长度为13则末尾补0</span></span><br><span class="line"><span class="keyword">for</span> idx, pic <span class="keyword">in</span> <span class="built_in">enumerate</span>(pics):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(pic) == <span class="number">13</span>:</span><br><span class="line">        pics[idx] = pic + <span class="string">&#x27;0&#x27;</span></span><br><span class="line"><span class="comment">#排序</span></span><br><span class="line">pics = <span class="built_in">sorted</span>(pics, key=<span class="keyword">lambda</span> x:<span class="built_in">int</span>(x))</span><br><span class="line">chapterpic_qian = re.findall(<span class="string">&#x27;\|jpg\|(\d&#123;4&#125;)&#x27;</span>, <span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line">chapterpic_hou = re.findall(<span class="string">&#x27;\|\|(\d&#123;5&#125;)&#x27;</span>, <span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> pic <span class="keyword">in</span> pics:</span><br><span class="line">	<span class="comment"># 末尾为0则去除0，因为0是之前添加的</span></span><br><span class="line">    <span class="keyword">if</span> pic[-<span class="number">1</span>] == <span class="string">&#x27;0&#x27;</span>:</span><br><span class="line">        url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/&#x27;</span> + chapterpic_qian + <span class="string">&#x27;/&#x27;</span> + chapterpic_hou + <span class="string">&#x27;/&#x27;</span> + pic[:-<span class="number">1</span>] + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/&#x27;</span> + chapterpic_qian + <span class="string">&#x27;/&#x27;</span> + chapterpic_hou + <span class="string">&#x27;/&#x27;</span> + pic + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(url)</span><br></pre></td></tr></table></figure>

<p><img src="/%5Cimg%5Cpython%5Cimage-20220908203104549.png" alt="image-20220908203104549"></p>
<p>跟网页的链接按顺序比对，你会发现没错！就是这个顺序！</p>
<h3 id="下载图片"><a href="#下载图片" class="headerlink" title="下载图片"></a>下载图片</h3><p>使用其中一个图片链接，用代码下载试试。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlretrieve</span><br><span class="line"></span><br><span class="line">dn_url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/3059/14237/14395217739069.jpg&#x27;</span></span><br><span class="line">urlretrieve(dn_url,<span class="string">&#x27;1.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>注：<em>通过<code>urlretrieve</code>方法，可以下载图片。第一个参数是 <code>下载链接</code>，第二个参数是下载后的 <code>文件保存名</code>。</em></p>
<p><code>特殊情况</code>：出现了HTTP Error，错误代码是403。403表示资源不可用，这是又是一种 <code>典型的反爬虫</code> 手段。</p>
<img src="\img\python\image-20220908203519991.png" alt="image-20220908203519991" style="zoom:75%;" />

<p>打开这个图片链接：<a target="_blank" rel="noopener" href="https://images.dmzj.com/img/chapterpic/3059/14237/14395217739069.jpg">https://images.dmzj.com/img/chapterpic/3059/14237/14395217739069.jpg</a></p>
<p>这个地址就是图片的真实地址，在浏览器中打开，可能直接无法打开，或者能打开，但是一刷新就又不能打开了！如果打开章节页面后，再打开这个图片链接就又能看到图片了。</p>
<p>记住，这就是一种典型的通过Referer的反爬虫手段！Referer可以理解为来路，先打开章节URL链接（<a target="_blank" rel="noopener" href="https://www.dmzj.com/view/yaoshenji/41917.html%EF%BC%89%EF%BC%8C%E5%86%8D%E6%89%93%E5%BC%80%E5%9B%BE%E7%89%87%E9%93%BE%E6%8E%A5%E3%80%82%E6%89%93%E5%BC%80%E5%9B%BE%E7%89%87%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8CReferer%E7%9A%84%E4%BF%A1%E6%81%AF%E9%87%8C%E4%BF%9D%E5%AD%98%E7%9A%84%E6%98%AF%E7%AB%A0%E8%8A%82URL%E3%80%82">https://www.dmzj.com/view/yaoshenji/41917.html），再打开图片链接。打开图片的时候，Referer的信息里保存的是章节URL。</a></p>
<p>动漫之家网站的做法就是，站内的用户访问这个图片，我就给他看，从其它地方过来的用户，我就不给他看。是不是站内用户，就是根据Referer进行简单的判断。</p>
<p>这就是很典型的，反爬虫手段！解决办法也简单，它需要啥，咱给它就完了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> closing</span><br><span class="line"></span><br><span class="line">download_header = &#123;</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://www.dmzj.com/view/yaoshenji/41917.html&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dn_url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/3059/14237/14395217739069.jpg&#x27;</span></span><br><span class="line"><span class="keyword">with</span> closing(requests.get(dn_url, headers = download_header, stream = <span class="literal">True</span>)) <span class="keyword">as</span> response:</span><br><span class="line">    chunk_size = <span class="number">1024</span>  </span><br><span class="line">    content_size = <span class="built_in">int</span>(response.headers[<span class="string">&#x27;content-length&#x27;</span>])  </span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;文件大小:%0.2f KB&#x27;</span> % (content_size / chunk_size))</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;1.jpg&#x27;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> file:  </span><br><span class="line">            <span class="keyword">for</span> data <span class="keyword">in</span> response.iter_content(chunk_size=chunk_size):  </span><br><span class="line">                file.write(data)  </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;链接异常&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;下载完成！&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>注：<em>使用<code>closing</code>方法可以设置Headers信息，这个Headers信息里保存Referer来路，就是第一章的URL，最后以写文件的形式，保存这个图片。</em></p>
<h3 id="整合代码-1"><a href="#整合代码-1" class="headerlink" title="整合代码"></a>整合代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> closing</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建保存目录</span></span><br><span class="line">save_dir = <span class="string">&#x27;妖神记&#x27;</span></span><br><span class="line"><span class="keyword">if</span> save_dir <span class="keyword">not</span> <span class="keyword">in</span> os.listdir(<span class="string">&#x27;./&#x27;</span>):</span><br><span class="line">    <span class="comment">#列出当前执行路径目录下的文件</span></span><br><span class="line">    os.mkdir(save_dir)</span><br><span class="line"></span><br><span class="line">target_url = <span class="string">&quot;https://www.dmzj.com/info/yaoshenji.html&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取动漫章节链接和章节名</span></span><br><span class="line">r = requests.get(url = target_url)</span><br><span class="line">bs = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">list_con_li = bs.find(<span class="string">&#x27;ul&#x27;</span>, class_=<span class="string">&quot;list_con_li&quot;</span>)</span><br><span class="line">cartoon_list = list_con_li.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">chapter_names = []</span><br><span class="line">chapter_urls = []</span><br><span class="line"><span class="keyword">for</span> cartoon <span class="keyword">in</span> cartoon_list:</span><br><span class="line">    href = cartoon.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    name = cartoon.text</span><br><span class="line">    chapter_names.insert(<span class="number">0</span>, name)</span><br><span class="line">    chapter_urls.insert(<span class="number">0</span>, href)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载漫画 </span></span><br><span class="line"><span class="keyword">for</span> i, url <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(chapter_urls)):</span><br><span class="line">    download_header = &#123;</span><br><span class="line">        <span class="string">&#x27;Referer&#x27;</span>: url</span><br><span class="line">    &#125;</span><br><span class="line">    name = chapter_names[i]</span><br><span class="line">    <span class="comment"># 去掉.</span></span><br><span class="line">    <span class="keyword">while</span> <span class="string">&#x27;.&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        name = name.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    chapter_save_dir = os.path.join(save_dir, name)</span><br><span class="line">    <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> os.listdir(save_dir):</span><br><span class="line">        os.mkdir(chapter_save_dir)</span><br><span class="line">        r = requests.get(url = url)</span><br><span class="line">        html = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">        script_info = html.script</span><br><span class="line">        pics = re.findall(<span class="string">&#x27;\d&#123;13,14&#125;&#x27;</span>, <span class="built_in">str</span>(script_info))</span><br><span class="line">        <span class="keyword">for</span> j, pic <span class="keyword">in</span> <span class="built_in">enumerate</span>(pics):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(pic) == <span class="number">13</span>:</span><br><span class="line">                pics[j] = pic + <span class="string">&#x27;0&#x27;</span></span><br><span class="line">        pics = <span class="built_in">sorted</span>(pics, key=<span class="keyword">lambda</span> x:<span class="built_in">int</span>(x))</span><br><span class="line">        chapterpic_hou = re.findall(<span class="string">&#x27;\|(\d&#123;5&#125;)\|&#x27;</span>, <span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line">        chapterpic_qian = re.findall(<span class="string">&#x27;\|(\d&#123;4&#125;)\|&#x27;</span>, <span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> idx, pic <span class="keyword">in</span> <span class="built_in">enumerate</span>(pics):</span><br><span class="line">            <span class="keyword">if</span> pic[-<span class="number">1</span>] == <span class="string">&#x27;0&#x27;</span>:</span><br><span class="line">                url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/&#x27;</span> + chapterpic_qian + <span class="string">&#x27;/&#x27;</span> + chapterpic_hou + <span class="string">&#x27;/&#x27;</span> + pic[:-<span class="number">1</span>] + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/&#x27;</span> + chapterpic_qian + <span class="string">&#x27;/&#x27;</span> + chapterpic_hou + <span class="string">&#x27;/&#x27;</span> + pic + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">            pic_name = <span class="string">&#x27;%03d.jpg&#x27;</span> % (idx + <span class="number">1</span>)</span><br><span class="line">            pic_save_path = os.path.join(chapter_save_dir, pic_name)</span><br><span class="line">            <span class="keyword">with</span> closing(requests.get(url, headers = download_header, stream = <span class="literal">True</span>)) <span class="keyword">as</span> response:  </span><br><span class="line">                chunk_size = <span class="number">1024</span>  </span><br><span class="line">                content_size = <span class="built_in">int</span>(response.headers[<span class="string">&#x27;content-length&#x27;</span>])  </span><br><span class="line">                <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                    <span class="keyword">with</span> <span class="built_in">open</span>(pic_save_path, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> file:  </span><br><span class="line">                        <span class="keyword">for</span> data <span class="keyword">in</span> response.iter_content(chunk_size=chunk_size):  </span><br><span class="line">                            file.write(data)  </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;链接异常&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>



<h2 id="实例：爬取视频"><a href="#实例：爬取视频" class="headerlink" title="实例：爬取视频"></a>实例：爬取视频</h2><h3 id="查找资源"><a href="#查找资源" class="headerlink" title="查找资源"></a>查找资源</h3><p>首先得找到资源，采用站内搜索很简单。抓搜索的请求包也特别简单，教一个小技巧：第一步输入要搜索的内容，第二步打开并清理 Network，第三步点击搜索按钮。</p>
<img src="\img\python\image-20220909092804868.png" alt="image-20220909092804868" style="zoom:80%;" />

<p>Network 里第一个弹出的就是搜索的请求包。</p>
<img src="\img\python\image-20220909092914563.png" alt="image-20220909092914563" style="zoom:80%;" />

<p>可以看到，这是一个 POST 请求，之前遇到的都是get请求。GET 请求，就是字面意思，从服务器获取数据。POST 请求，也是字面意思，给服务器发送数据。</p>
<p>因为我们是搜索嘛，得告诉服务器，咱搜索啥，给服务器传递数据，就是通过 POST。能 POST 的数据有很多，可以是简单的字符串、也可以是一张图片。POST 啥数据，那就看服务器要啥数据，要啥给啥，就这么简单。</p>
<p>看一下搜索的结果页，不难发现，搜索结果存储在 class 属性为imgs的div标签里。</p>
<img src="\img\python\image-20220909093248950.png" alt="image-20220909093248950" style="zoom:80%;" />

<p>根据浏览器抓包结果，填写 data 信息即可。由于网站也有简单的 Header 反爬虫，所以一些必要的 Headers 信息也要填写。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">search_keyword = <span class="string">&#x27;鬼灭之刃第二季&#x27;</span></span><br><span class="line">search_url = <span class="string">&#x27;http://www.yinghuaz.com/search/-------------.html&#x27;</span></span><br><span class="line"><span class="comment">#http://www.szycjbs.com/search/-------------.html</span></span><br><span class="line"><span class="comment">#http://www.jisudhw.com/index.php?m=vod-search</span></span><br><span class="line"><span class="comment">#serach_params = &#123;</span></span><br><span class="line">    <span class="comment">#&#x27;m&#x27;: &#x27;vod-search&#x27;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line">serach_headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;http://www.yinghuaz.com/search/-------------.html&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Origin&#x27;</span>: <span class="string">&#x27;http://www.yinghuaz.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;www.yinghuaz.com&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">serach_datas = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span>: search_keyword,</span><br><span class="line">    <span class="string">&#x27;submit&#x27;</span>: <span class="string">&#x27;search&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">r = requests.post(url=search_url, headers=serach_headers, data=serach_datas)</span><br><span class="line"><span class="comment">#r = requests.post(url=search_url, params=serach_params, headers=serach_headers, data=serach_datas)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#获取搜索结果</span></span><br><span class="line">r.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">server = <span class="string">&#x27;http://www.yinghuaz.com&#x27;</span></span><br><span class="line">search_html = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">search_lists = search_html.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;imgs&#x27;</span>)</span><br><span class="line">url = server + search_lists.find(<span class="string">&#x27;p&#x27;</span>).find(<span class="string">&#x27;a&#x27;</span>).get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">name = search_lists.find(<span class="string">&#x27;p&#x27;</span>).find(<span class="string">&#x27;a&#x27;</span>).string</span><br><span class="line"><span class="built_in">print</span>(url + <span class="string">&#x27;&#x27;</span> +name)</span><br></pre></td></tr></table></figure>

<img src="\img\python\image-20220909094925586.png" alt="image-20220909094925586" style="zoom:80%;" />



<h3 id="获取每集链接"><a href="#获取每集链接" class="headerlink" title="获取每集链接"></a>获取每集链接</h3><p>在详情页，视频链接都放在了id属性为play_1的div里面</p>
<img src="\img\python\image-20220909095115348.png" alt="image-20220909095115348" style="zoom:80%;" />

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">url=<span class="string">&#x27;http://www.yinghuaz.com/view/2391.html鬼灭之刃第二季&#x27;</span></span><br><span class="line">req = requests.get(url = url)</span><br><span class="line">req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">bs = BeautifulSoup(req.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">bs = bs.find(<span class="string">&quot;div&quot;</span>,&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;play_1&#x27;</span>&#125;)</span><br><span class="line">episodes = bs.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">num = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> episodes:</span><br><span class="line">    url = server + episode.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;第%0d集：&#x27;</span>%num + <span class="string">&#x27; &#x27;</span> + url)</span><br><span class="line">    num+=<span class="number">1</span></span><br></pre></td></tr></table></figure>

<img src="\img\python\image-20220909095249092.png" alt="image-20220909095249092" style="zoom:80%;" />



<h3 id="获取每集视频对应的链接"><a href="#获取每集视频对应的链接" class="headerlink" title="获取每集视频对应的链接"></a>获取每集视频对应的链接</h3><p>我们发现每一集视频都在id为WANG的iframe标签中，但通过view-source查看源码时发现iframe不存在，但id为playbox的div中也存在视频链接，只是缺少前面一部分，补齐即可。</p>
<p><img src="/%5Cimg%5Cpython%5Cimage-20220909101857256.png" alt="image-20220909101857256"></p>
<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220909104313121.png" alt="image-20220909104313121" style="zoom:80%;" />

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> episodes:</span><br><span class="line">    url = server + episode.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    req = requests.get(url = url)</span><br><span class="line">    bs = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    content = <span class="string">&#x27;https://jx.wpng.cc/m.php?url=&#x27;</span> + bs.find(<span class="string">&#x27;div&#x27;</span>,&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;playbox&#x27;</span>&#125;).get(<span class="string">&#x27;data-vid&#x27;</span>)</span><br><span class="line">	<span class="comment"># 第11集视频链接少了两位&#x27;u8&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(content)==<span class="number">103</span>:</span><br><span class="line">        content+=<span class="string">&#x27;u8&#x27;</span></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;视频&#x27;</span> + <span class="built_in">str</span>(num) + <span class="string">&#x27;链接：&#x27;</span> + <span class="string">&#x27; &#x27;</span> + content)</span><br><span class="line">    num+=<span class="number">1</span></span><br></pre></td></tr></table></figure>

<img src="\img\python\image-20220909111655460.png" alt="image-20220909111655460" style="zoom:80%;" />

<h3 id="下载视频"><a href="#下载视频" class="headerlink" title="下载视频"></a>下载视频</h3><ol>
<li><p>视频下载，一般而言，无非两种情况。</p>
<blockquote>
<ul>
<li>链接明确是以 mp4、mkv、rmvb 这类视频格式后缀为结尾的链接，这种下载很简单，跟图片下载方法一样，就是视频文件要比图片大而已。</li>
<li>链接是以 m3u8 这类分段视频后缀结尾的链接。现在很多视频都是分段存储的，你看视频的时候，其实是在加载一个个 ts 视频片段，一个片段是几秒钟的视频。</li>
</ul>
</blockquote>
<p>这种视频要怎么下载？怎么将 ts 视频片段组合成一个视频？</p>
<p>m3u8 这种格式的视频，就是由一个个 ts 视频片段组成的。一个 m3u8 文件并不大，你可以把它理解为链表，每个 ts 视频片段文件，都有下一个时序的 ts 视频片段的地址。</p>
<p>记住一点，解决音频和视频的一些问题，可以看看 <code>FFmpeg</code>，它的中文名叫多媒体视频处理工具。FFmpeg 有非常强大的功能包括视频采集、视频格式转换、视频抓图、给视频加水印等功能。这种 ts 视频片段合成，格式转换问题，交给 FFmpeg 就好了。</p>
</li>
<li><p>安装FFmpeg：<code>pip install ffmpy3</code></p>
<ul>
<li><p>下载 m3u8 文件，URL：<a target="_blank" rel="noopener" href="https://xxx.xxx.com/xxx.m3u8%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%9C%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%BE%93%E5%85%A5%E4%B8%8B%E6%96%B9%E7%9A%84%E6%8C%87%E4%BB%A4%EF%BC%8C%E8%A7%86%E9%A2%91%E4%BC%9A%E4%BF%9D%E5%AD%98%E4%B8%BA%E3%80%8C%E7%AC%AC001%E9%9B%86.mp4%E3%80%8D%E3%80%82">https://xxx.xxx.com/xxx.m3u8，可以在命令行输入下方的指令，视频会保存为「第001集.mp4」。</a></p>
<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">ffmpeg -i &quot;http://youku.com-youku.<span class="built_in">net</span>/<span class="number">20180614</span>/<span class="number">11920</span>_4c9e1cc1/index.m3u8&quot; &quot;第<span class="number">001</span>集.mp4&quot;</span><br></pre></td></tr></table></figure>

<p>如果使用 Python 接口，使用以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> ffmpy3</span><br><span class="line"></span><br><span class="line">exe = <span class="string">r&#x27;F:\学习\Python\ffmpeg-master-latest-win64-lgpl-shared\bin\ffmpeg.exe&#x27;</span></span><br><span class="line">ff = ffmpy3.FFmpeg(executable=exe, inputs=&#123;<span class="string">&#x27;http://youku.com-youku.net/20180614/11920_4c9e1cc1/index.m3u8&#x27;</span>:<span class="literal">None</span>&#125;,outputs=&#123;<span class="string">&#x27;第1集.mp4&#x27;</span>:<span class="literal">None</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(ff.cmd)</span><br><span class="line">ff.run()</span><br></pre></td></tr></table></figure>

<p>注：<em>FFmpeg 自动整合 ts 分段视频，并保存为 mp4 格式的视频。</em></p>
<p>注：<em>FFmpeg.exe下载地址：<a target="_blank" rel="noopener" href="https://github.com/BtbN/FFmpeg-Builds/releases">https://github.com/BtbN/FFmpeg-Builds/releases</a></em></p>
</li>
<li><p>对于MP4类型的视频</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> ffmpy3</span><br><span class="line"></span><br><span class="line">exe = <span class="string">r&#x27;F:\学习\Python\ffmpeg-master-latest-win64-lgpl-shared\bin\ffmpeg.exe&#x27;</span></span><br><span class="line">ff = ffmpy3.FFmpeg(executable=exe, inputs=&#123;<span class="string">&#x27;https://ali-ad.a.yximgs.com/bs2/ad-creative-center-temp/1b8e681bf6284f8dbb5bd7eb56eb27c7.mp4&#x27;</span>:<span class="literal">None</span>&#125;,outputs=&#123;<span class="string">&#x27;第2集.mp4&#x27;</span>:<span class="literal">None</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(ff.cmd)</span><br><span class="line">ff.run()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="整合代码-2"><a href="#整合代码-2" class="headerlink" title="整合代码"></a>整合代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> ffmpy3</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool <span class="keyword">as</span> ThreadPool</span><br><span class="line"></span><br><span class="line">search_keyword = <span class="string">&#x27;鬼灭之刃第二季&#x27;</span></span><br><span class="line">search_url = <span class="string">&#x27;http://www.yinghuaz.com/search/-------------.html&#x27;</span></span><br><span class="line">serach_headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;http://www.yinghuaz.com/search/-------------.html&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Origin&#x27;</span>: <span class="string">&#x27;http://www.yinghuaz.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;www.yinghuaz.com&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">serach_datas = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span>: search_keyword,</span><br><span class="line">    <span class="string">&#x27;submit&#x27;</span>: <span class="string">&#x27;search&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">r = requests.post(url=search_url, headers=serach_headers, data=serach_datas)</span><br><span class="line">r.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">server = <span class="string">&#x27;http://www.yinghuaz.com&#x27;</span></span><br><span class="line">search_html = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">search_lists = search_html.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;imgs&#x27;</span>)</span><br><span class="line">url = server + search_lists.find(<span class="string">&#x27;p&#x27;</span>).find(<span class="string">&#x27;a&#x27;</span>).get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">name = search_lists.find(<span class="string">&#x27;p&#x27;</span>).find(<span class="string">&#x27;a&#x27;</span>).string</span><br><span class="line"><span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> os.listdir(<span class="string">&#x27;./&#x27;</span>):</span><br><span class="line">    os.mkdir(name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取每一集的url</span></span><br><span class="line">req = requests.get(url = url)</span><br><span class="line">req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">bs = BeautifulSoup(req.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">bs = bs.find(<span class="string">&quot;div&quot;</span>,&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;play_1&#x27;</span>&#125;)</span><br><span class="line">episodes = bs.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">num = <span class="number">1</span></span><br><span class="line">serach_res = &#123;&#125;</span><br><span class="line">exe = <span class="string">r&#x27;F:\学习\Python\ffmpeg-master-latest-win64-lgpl-shared\bin\ffmpeg.exe&#x27;</span></span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> episodes:</span><br><span class="line">    <span class="comment"># 每一集的url</span></span><br><span class="line">    url = server + episode.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    req = requests.get(url = url)</span><br><span class="line">    bs = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    <span class="comment"># 每一集视频的url</span></span><br><span class="line">    url = <span class="string">&#x27;https://jx.wpng.cc/m.php?url=&#x27;</span> + bs.find(<span class="string">&#x27;div&#x27;</span>,&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;playbox&#x27;</span>&#125;).get(<span class="string">&#x27;data-vid&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(url)==<span class="number">103</span>:</span><br><span class="line">        url+=<span class="string">&#x27;u8&#x27;</span></span><br><span class="line">    serach_res[url] = num</span><br><span class="line">    num+=<span class="number">1</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">downVideo</span>(<span class="params">url</span>):</span><br><span class="line">    num = serach_res[url]</span><br><span class="line">    name = os.path.join(video_dir,<span class="string">&#x27;第%02d集.mp4&#x27;</span> %num)</span><br><span class="line">    ff = ffmpy3.FFmpeg(executable=exe, inputs=&#123;url:<span class="literal">None</span>&#125;,outputs=&#123;name:<span class="literal">None</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(ff.cmd)</span><br><span class="line">    ff.run()</span><br><span class="line"></span><br><span class="line"><span class="comment">#开8个线程    </span></span><br><span class="line">pool = ThreadPool(<span class="number">8</span>)</span><br><span class="line">results = pool.<span class="built_in">map</span>(downVideo,serach_res.keys())</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure>



<h3 id="特殊情况——非真实链接"><a href="#特殊情况——非真实链接" class="headerlink" title="特殊情况——非真实链接"></a>特殊情况——非真实链接</h3><p>当我们得到一个m3u8各式的视频链接时，例如：<a target="_blank" rel="noopener" href="https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8">https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8</a></p>
<ol>
<li>测试该地址，运行下方代码，从结果可以看出这是一个嵌套的地址。</li>
</ol>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">playlist = m3u8.load(uri=<span class="string">&#x27;https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8&#x27;</span>, headers=headers)</span><br><span class="line"><span class="built_in">print</span>(playlist.data)</span><br></pre></td></tr></table></figure>

<p>  <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220909123046195.png" alt="image-20220909123046195"></p>
<ol start="2">
<li>解析真实地址，运行下方代码，结果为：<code>https://mgtv.sd-play.com/20211006/UiC7BSo5/1000kb/hls/index.m3u8</code></li>
</ol>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_real_url</span>(<span class="params">url</span>):</span><br><span class="line">    playlist = m3u8.load(uri=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> playlist.playlists[<span class="number">0</span>].absolute_uri</span><br><span class="line"></span><br><span class="line">real_url = get_real_url(<span class="string">&#x27;https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(real_url)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>解析真实地址的加密key，运行代码后可以看到密钥下载地址和加密类型，结果为：<code>https://mgtv.shanshanku.com/20211006/UiC7BSo5/1000kb/hls/key.key AES-128 None</code></li>
</ol>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">playlist = m3u8.load(uri=real_url, headers=headers)</span><br><span class="line">key = playlist.keys[-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(key.uri, key.method, key.iv)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>使用request下载密钥，运行结果为：<code>b&#39;888288aee8b98ae9&#39;</code></li>
</ol>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(playlist.keys[<span class="number">0</span>].uri, headers=headers)</span><br><span class="line">key = r.content</span><br><span class="line"><span class="built_in">print</span>(key)</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>可以单线程直接下载视频，整合代码</li>
</ol>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> Crypto.Cipher <span class="keyword">import</span> AES</span><br><span class="line"><span class="keyword">from</span> Crypto.Util.Padding <span class="keyword">import</span> pad</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_real_url</span>(<span class="params">url</span>):</span><br><span class="line">    playlist = m3u8.load(uri=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> playlist.playlists[<span class="number">0</span>].absolute_uri</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AESDecrypt</span>(<span class="params">cipher_text, key, iv</span>):</span><br><span class="line">    cipher_text = pad(data_to_pad=cipher_text, block_size=AES.block_size)</span><br><span class="line">    aes = AES.new(key=key, mode=AES.MODE_CBC, iv=key)</span><br><span class="line">    cipher_text = aes.decrypt(cipher_text)</span><br><span class="line">    <span class="keyword">return</span> cipher_text</span><br><span class="line"></span><br><span class="line">real_url = get_real_url(<span class="string">&#x27;https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">playlist = m3u8.load(uri=real_url, headers=headers)</span><br><span class="line">key = playlist.keys[-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(key.uri, key.method, key.iv)</span><br><span class="line"></span><br><span class="line">r = requests.get(playlist.keys[<span class="number">0</span>].uri, headers=headers)</span><br><span class="line">key = r.content</span><br><span class="line"><span class="built_in">print</span>(key)</span><br><span class="line"></span><br><span class="line">n = <span class="built_in">len</span>(playlist.segments)</span><br><span class="line"><span class="comment">#playlist.segments中存储的时各个ts片段</span></span><br><span class="line">size = <span class="number">0</span></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> i, seg <span class="keyword">in</span> <span class="built_in">enumerate</span>(playlist.segments, <span class="number">1</span>):</span><br><span class="line">    r = requests.get(seg.absolute_uri, headers=headers)</span><br><span class="line">    data = r.content</span><br><span class="line">    data = AESDecrypt(data, key=key, iv=key)</span><br><span class="line">    size += <span class="built_in">len</span>(data)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;reusult.mp4&quot;</span>, <span class="string">&quot;ab&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\r下载进度(<span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;n&#125;</span>)，已下载：<span class="subst">&#123;size/<span class="number">1024</span>/<span class="number">1024</span>:<span class="number">.2</span>f&#125;</span>MB，下载已耗时：<span class="subst">&#123;time.time()-start:<span class="number">.2</span>f&#125;</span>s&quot;</span>, end=<span class="string">&quot; &quot;</span>)</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>封装函数</li>
</ol>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> Crypto.Util.Padding <span class="keyword">import</span> pad</span><br><span class="line"><span class="keyword">from</span> Crypto.Cipher <span class="keyword">import</span> AES</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_real_url</span>(<span class="params">url</span>):</span><br><span class="line">    playlist = m3u8.load(uri=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> playlist.playlists[<span class="number">0</span>].absolute_uri</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AESDecrypt</span>(<span class="params">cipher_text, key, iv</span>):</span><br><span class="line">    cipher_text = pad(data_to_pad=cipher_text, block_size=AES.block_size)</span><br><span class="line">    aes = AES.new(key=key, mode=AES.MODE_CBC, iv=key)</span><br><span class="line">    cipher_text = aes.decrypt(cipher_text)</span><br><span class="line">    <span class="keyword">return</span> cipher_text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_m3u8_video</span>(<span class="params">url, save_name</span>):</span><br><span class="line">    real_url = get_real_url(url)</span><br><span class="line">    playlist = m3u8.load(uri=real_url, headers=headers)</span><br><span class="line">    key = requests.get(playlist.keys[-<span class="number">1</span>].uri, headers=headers).content</span><br><span class="line"></span><br><span class="line">    n = <span class="built_in">len</span>(playlist.segments)</span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> i, seg <span class="keyword">in</span> <span class="built_in">enumerate</span>(playlist.segments, <span class="number">1</span>):</span><br><span class="line">        r = requests.get(seg.absolute_uri, headers=headers)</span><br><span class="line">        data = r.content</span><br><span class="line">        data = AESDecrypt(data, key=key, iv=key)</span><br><span class="line">        size += <span class="built_in">len</span>(data)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(save_name, <span class="string">&quot;ab&quot;</span> <span class="keyword">if</span> i != <span class="number">1</span> <span class="keyword">else</span> <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(data)</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">f&quot;\r下载进度(<span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;n&#125;</span>)，已下载：<span class="subst">&#123;size/<span class="number">1024</span>/<span class="number">1024</span>:<span class="number">.2</span>f&#125;</span>MB，下载已耗时：<span class="subst">&#123;time.time()-start:<span class="number">.2</span>f&#125;</span>s&quot;</span>, end=<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line">download_m3u8_video(<span class="string">&#x27;https://vod8.wenshibaowenbei.com/20210628/g4yNLlI7/index.m3u8&#x27;</span>, <span class="string">&#x27;走进家门.mp4&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ol start="7">
<li><p>多线程下载改造</p>
<p>对于多线程，由于下载的文件可能出现间断，所以我们不能直接追加到目标视频中，可以先下下来，最后统一合并并删除。</p>
</li>
</ol>
<ul>
<li><p>先创建ts视频下载的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_ts</span>(<span class="params">url, key, i</span>):</span><br><span class="line">    r = requests.get(url, headers=headers)</span><br><span class="line">    data = r.content</span><br><span class="line">    data = AESDecrypt(data, key=key, iv=key)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&quot;tmp/<span class="subst">&#123;i:<span class="number">0</span>&gt;5d&#125;</span>.ts&quot;</span>, <span class="string">&quot;ab&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\r<span class="subst">&#123;i:<span class="number">0</span>&gt;5d&#125;</span>.ts已下载&quot;</span>, end=<span class="string">&quot;  &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;tmp&quot;</span>):</span><br><span class="line">    os.mkdir(<span class="string">&#x27;tmp&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>任意下载一个片段测试一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_real_url</span>(<span class="params">url</span>):</span><br><span class="line">    playlist = m3u8.load(uri=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> playlist.playlists[<span class="number">0</span>].absolute_uri</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">real_url = get_real_url(</span><br><span class="line">    <span class="string">&#x27;https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8&#x27;</span>)</span><br><span class="line">playlist = m3u8.load(uri=real_url, headers=headers)</span><br><span class="line">key = requests.get(playlist.keys[-<span class="number">1</span>].uri, headers=headers).content</span><br><span class="line"></span><br><span class="line">download_ts(playlist.segments[<span class="number">0</span>].absolute_uri, key, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>检查该片段可以正常播放。</p>
</li>
<li><p>然后执行以下方法即可10个线程同时一起下载：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> ThreadPoolExecutor(max_workers=<span class="number">10</span>) <span class="keyword">as</span> pool:</span><br><span class="line">    <span class="keyword">for</span> i, seg <span class="keyword">in</span> <span class="built_in">enumerate</span>(playlist.segments):</span><br><span class="line">        pool.submit(download_ts, seg.absolute_uri, key, i)</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后我们实现文件的合并和ts临时文件清除：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;video.mp4&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">    files = glob.glob(<span class="string">&#x27;tmp/*.ts&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">            fw.write(fr.read())</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;\r<span class="subst">&#123;file&#125;</span>已合并!总数:<span class="subst">&#123;<span class="built_in">len</span>(files)&#125;</span>&#x27;</span>, end=<span class="string">&quot;     &quot;</span>)</span><br><span class="line">        os.remove(file)</span><br></pre></td></tr></table></figure>
</li>
<li><p>完整代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> Crypto.Util.Padding <span class="keyword">import</span> pad</span><br><span class="line"><span class="keyword">from</span> Crypto.Cipher <span class="keyword">import</span> AES</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_ts</span>(<span class="params">url, key, i</span>):</span><br><span class="line">    r = requests.get(url, headers=headers)</span><br><span class="line">    data = r.content</span><br><span class="line">    data = AESDecrypt(data, key=key, iv=key)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&quot;tmp/<span class="subst">&#123;i:<span class="number">0</span>&gt;5d&#125;</span>.ts&quot;</span>, <span class="string">&quot;ab&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\r<span class="subst">&#123;i:<span class="number">0</span>&gt;5d&#125;</span>.ts已下载&quot;</span>, end=<span class="string">&quot;  &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_real_url</span>(<span class="params">url</span>):</span><br><span class="line">    playlist = m3u8.load(uri=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> playlist.playlists[<span class="number">0</span>].absolute_uri</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AESDecrypt</span>(<span class="params">cipher_text, key, iv</span>):</span><br><span class="line">    cipher_text = pad(data_to_pad=cipher_text, block_size=AES.block_size)</span><br><span class="line">    aes = AES.new(key=key, mode=AES.MODE_CBC, iv=key)</span><br><span class="line">    cipher_text = aes.decrypt(cipher_text)</span><br><span class="line">    <span class="keyword">return</span> cipher_text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_m3u8_video</span>(<span class="params">url, save_name, max_workers=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;tmp&quot;</span>):</span><br><span class="line">        os.mkdir(<span class="string">&#x27;tmp&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    real_url = get_real_url(url)</span><br><span class="line">    playlist = m3u8.load(uri=real_url, headers=headers)</span><br><span class="line">    key = requests.get(playlist.keys[-<span class="number">1</span>].uri, headers=headers).content</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ThreadPoolExecutor(max_workers=max_workers) <span class="keyword">as</span> pool:</span><br><span class="line">        <span class="keyword">for</span> i, seg <span class="keyword">in</span> <span class="built_in">enumerate</span>(playlist.segments):</span><br><span class="line">            pool.submit(download_ts, seg.absolute_uri, key, i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(save_name, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        files = glob.glob(<span class="string">&#x27;tmp/*.ts&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">                fw.write(fr.read())</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;\r<span class="subst">&#123;file&#125;</span>已合并!总数:<span class="subst">&#123;<span class="built_in">len</span>(files)&#125;</span>&#x27;</span>, end=<span class="string">&quot;     &quot;</span>)</span><br><span class="line">            os.remove(file)</span><br><span class="line"></span><br><span class="line">download_m3u8_video(<span class="string">&#x27;https://vod8.wenshibaowenbei.com/20210628/g4yNLlI7/index.m3u8&#x27;</span>, <span class="string">&#x27;走进家门.mp4&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="自动化测试工具-Selenium"><a href="#自动化测试工具-Selenium" class="headerlink" title="自动化测试工具 Selenium"></a>自动化测试工具 Selenium</h2><h3 id="安装Selenium"><a href="#安装Selenium" class="headerlink" title="安装Selenium"></a>安装Selenium</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install selenium</span><br></pre></td></tr></table></figure>



<h3 id="安装浏览器驱动"><a href="#安装浏览器驱动" class="headerlink" title="安装浏览器驱动"></a>安装浏览器驱动</h3><p><a target="_blank" rel="noopener" href="http://chromedriver.storage.googleapis.com/index.html">http://chromedriver.storage.googleapis.com/index.html</a></p>
<p>注：<em>安装时需要注意浏览器版本对应。</em></p>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>打开 <a target="_blank" rel="noopener" href="http://www.python.org/">www.python.org</a> 官网，并根据 name 属性为 q 找到搜索框，并输入 pycon 并点击搜索。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    driver = webdriver.Chrome(<span class="string">&quot;F:\学习\Python\chromedriver.exe&quot;</span>)</span><br><span class="line">    driver.get(<span class="string">&quot;https://www.python.org&quot;</span>)</span><br><span class="line">    <span class="keyword">assert</span> <span class="string">&quot;Python&quot;</span> <span class="keyword">in</span> driver.title</span><br><span class="line">    elem = driver.find_element_by_name(<span class="string">&quot;q&quot;</span>)</span><br><span class="line">    elem.send_keys(<span class="string">&quot;pycon&quot;</span>)</span><br><span class="line">    elem.send_keys(Keys.RETURN)</span><br><span class="line">    <span class="built_in">print</span>(driver.page_source)</span><br></pre></td></tr></table></figure>



<h3 id="find-element-by"><a href="#find-element-by" class="headerlink" title="find_element_by_*"></a>find_element_by_*</h3><p>find_element_by_*是一种定位网页元素的方法，有很多方式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">find_element_by_id</span><br><span class="line">find_element_by_name</span><br><span class="line">find_element_by_xpath</span><br><span class="line">find_element_by_link_text</span><br><span class="line">find_element_by_partial_link_text</span><br><span class="line">find_element_by_tag_name</span><br><span class="line">find_element_by_class_name</span><br><span class="line">find_element_by_css_selector</span><br></pre></td></tr></table></figure>

<p>可以通过，标签的 id 属性、name 属性、class_name 属性查找元素，也可以通过 xpath 等，其中用到最多的就是 xpath。举个例子，比如想找到 baidu.com 的搜索框的元素：</p>
<img src="\img\python\image-20220908230130072.png" alt="image-20220908230130072" style="zoom:67%;" />

<p>在搜索框位置，点击右键，选择 copy 下的 copy xpath，直接复制 xpath 。粘贴出来你会看到如下内容：<code>//*[@id=&quot;kw&quot;]</code></p>
<p>其实意思就是从根目录开始找，找到 id 属性为 kw 的标签。定位到搜索框，就可以通过百度输入想搜索的内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    driver = webdriver.Chrome(<span class="string">&quot;F:\学习\Python\chromedriver.exe&quot;</span>)</span><br><span class="line">    driver.get(<span class="string">&quot;https://www.baidu.com&quot;</span>)</span><br><span class="line">    elem = driver.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;kw&quot;]&#x27;</span>)</span><br><span class="line">    elem.send_keys(<span class="string">&quot;剑来&quot;</span>)</span><br><span class="line">    elem.send_keys(Keys.RETURN)</span><br></pre></td></tr></table></figure>



<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>关于 Selenuim 其他的基本方法和 Xpath 的基础知识参考</p>
<blockquote>
<ul>
<li>文章地址：<a target="_blank" rel="noopener" href="https://blog.csdn.net/c406495762/article/details/72331737">https://blog.csdn.net/c406495762/article/details/72331737</a></li>
<li>详细关于 Selenium 的 API 文档，可以查看官方手册：<a target="_blank" rel="noopener" href="https://selenium-python.readthedocs.io/index.html">https://selenium-python.readthedocs.io/index.html</a></li>
</ul>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">小辑轻舟</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/09/15/Python%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB/">http://example.com/2022/09/15/Python%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">知行记</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/09/15/%E5%9F%BA%E4%BA%8Ehexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B/"><img class="next-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">基于hexo博客搭建过程</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">小辑轻舟</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8"><span class="toc-number">2.</span> <span class="toc-text">使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%EF%BC%9A%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4"><span class="toc-number">3.</span> <span class="toc-text">实例：爬取小说</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5%E5%86%85%E5%AE%B9"><span class="toc-number">3.1.</span> <span class="toc-text">爬取网页内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E7%AB%A0%E8%8A%82%E9%93%BE%E6%8E%A5%E5%8F%8A%E7%AB%A0%E8%8A%82%E5%90%8D"><span class="toc-number">3.2.</span> <span class="toc-text">获取章节链接及章节名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E4%BB%A3%E7%A0%81"><span class="toc-number">3.3.</span> <span class="toc-text">整合代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%EF%BC%9A%E7%88%AC%E5%8F%96%E6%BC%AB%E7%94%BB"><span class="toc-number">4.</span> <span class="toc-text">实例：爬取漫画</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E7%AB%A0%E8%8A%82%E5%90%8D%E5%92%8C%E7%AB%A0%E8%8A%82%E9%93%BE%E6%8E%A5"><span class="toc-number">4.1.</span> <span class="toc-text">获取章节名和章节链接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%BC%AB%E7%94%BB%E5%9B%BE%E7%89%87%E5%9C%B0%E5%9D%80"><span class="toc-number">4.2.</span> <span class="toc-text">获取漫画图片地址</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87"><span class="toc-number">4.3.</span> <span class="toc-text">下载图片</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E4%BB%A3%E7%A0%81-1"><span class="toc-number">4.4.</span> <span class="toc-text">整合代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%EF%BC%9A%E7%88%AC%E5%8F%96%E8%A7%86%E9%A2%91"><span class="toc-number">5.</span> <span class="toc-text">实例：爬取视频</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E6%89%BE%E8%B5%84%E6%BA%90"><span class="toc-number">5.1.</span> <span class="toc-text">查找资源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%AF%8F%E9%9B%86%E9%93%BE%E6%8E%A5"><span class="toc-number">5.2.</span> <span class="toc-text">获取每集链接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%AF%8F%E9%9B%86%E8%A7%86%E9%A2%91%E5%AF%B9%E5%BA%94%E7%9A%84%E9%93%BE%E6%8E%A5"><span class="toc-number">5.3.</span> <span class="toc-text">获取每集视频对应的链接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E8%A7%86%E9%A2%91"><span class="toc-number">5.4.</span> <span class="toc-text">下载视频</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E4%BB%A3%E7%A0%81-2"><span class="toc-number">5.5.</span> <span class="toc-text">整合代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E6%AE%8A%E6%83%85%E5%86%B5%E2%80%94%E2%80%94%E9%9D%9E%E7%9C%9F%E5%AE%9E%E9%93%BE%E6%8E%A5"><span class="toc-number">5.6.</span> <span class="toc-text">特殊情况——非真实链接</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7-Selenium"><span class="toc-number">6.</span> <span class="toc-text">自动化测试工具 Selenium</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Selenium"><span class="toc-number">6.1.</span> <span class="toc-text">安装Selenium</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E6%B5%8F%E8%A7%88%E5%99%A8%E9%A9%B1%E5%8A%A8"><span class="toc-number">6.2.</span> <span class="toc-text">安装浏览器驱动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B"><span class="toc-number">6.3.</span> <span class="toc-text">实例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#find-element-by"><span class="toc-number">6.4.</span> <span class="toc-text">find_element_by_*</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">6.5.</span> <span class="toc-text">其他</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/09/15/Python%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB/" title="Python简单爬虫"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python简单爬虫"/></a><div class="content"><a class="title" href="/2022/09/15/Python%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB/" title="Python简单爬虫">Python简单爬虫</a><time datetime="2022-09-15T00:56:12.000Z" title="发表于 2022-09-15 08:56:12">2022-09-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/15/%E5%9F%BA%E4%BA%8Ehexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B/" title="基于hexo博客搭建过程"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="基于hexo博客搭建过程"/></a><div class="content"><a class="title" href="/2022/09/15/%E5%9F%BA%E4%BA%8Ehexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B/" title="基于hexo博客搭建过程">基于hexo博客搭建过程</a><time datetime="2022-09-14T23:21:29.000Z" title="发表于 2022-09-15 07:21:29">2022-09-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/14/hexo%E5%88%9D%E4%BD%93%E9%AA%8C/" title="hexo初体验"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hexo初体验"/></a><div class="content"><a class="title" href="/2022/09/14/hexo%E5%88%9D%E4%BD%93%E9%AA%8C/" title="hexo初体验">hexo初体验</a><time datetime="2022-09-14T14:39:28.000Z" title="发表于 2022-09-14 22:39:28">2022-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/14/hello-world/" title="Hello World"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2022/09/14/hello-world/" title="Hello World">Hello World</a><time datetime="2022-09-14T12:41:02.911Z" title="发表于 2022-09-14 20:41:02">2022-09-14</time></div></div></div></div></div></div></main><footer id="footer" style="background: linear-gradient( 135deg,"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 小辑轻舟</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>